{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1740381894720,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"0SQ2MroAIK8x"},"outputs":[],"source":["# On Kaggle\n","# 1. Attach a GPU (see recitation for details on how to do these steps.)\n","# 2. Enable file persistence (see recitation for details on how to do these steps.)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18194,"status":"ok","timestamp":1740381912915,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"a04b7ffSIK8x","outputId":"ed9b501c-f159-400e-fc27-19f498196c12"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Git/class_10623/hw2/handout\n","/content/drive/MyDrive/Git/class_10623/hw2/handout\n"]}],"source":["# Uncomment this on Colab\n","\n","# # Mount your Google Drive to Colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Git/class_10623/hw2/handout'\n","!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1740381912923,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"MspMKvOSIK8y"},"outputs":[],"source":["# Fetch the homework files.\n","# On Colab, if you save to your drive (by running %cd path_to_your_project), you only need to do this once.\n","#!wget http://www.cs.cmu.edu/~mgormley/courses/10423/homework/hw2.zip\n","#!unzip hw2.zip\n","#!mv handout/* ."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104129,"status":"ok","timestamp":1740382017053,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"1_Yl0V3oIK8y","outputId":"dfd06fcd-236d-45ed-c40d-37d1e8ba3f03"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# # Set up the environment\n","!pip install --quiet -r requirements.txt\n","\n","from utils import train_diffusion, visualize_diffusion"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":80,"status":"ok","timestamp":1740384068229,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"TgqMkV9cIK8y"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","import wandb\n","\n","def extract(a, t, x_shape):\n","    \"\"\"\n","    This function abstracts away the tedious indexing that would otherwise have\n","    to be done to properly compute the diffusion equations from lecture. This\n","    is necessary because we train data in batches, while the math taught in\n","    lecture only considers a single sample.\n","\n","    To use this function, consider the example\n","        alpha_t * x\n","    To compute this in code, we would write\n","        extract(alpha, t, x.shape) * x\n","\n","    Args:\n","        a: 1D tensor containing the value at each time step.\n","        t: 1D tensor containing a batch of time indices.\n","        x_shape: The reference shape.\n","    Returns:\n","        The extracted tensor.\n","    \"\"\"\n","    b, *_ = t.shape\n","    t = t.to(a.device)\n","    out = a.gather(-1, t)\n","    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n","\n","def cosine_schedule(timesteps, s=0.008):\n","    \"\"\"\n","    Passes the input timesteps through the cosine schedule for the diffusion process\n","    Args:\n","        timesteps: 1D tensor containing a batch of time indices.\n","        s: The strength of the schedule.\n","    Returns:\n","        1D tensor of the same shape as timesteps, with the computed alpha.\n","    \"\"\"\n","    steps = timesteps + 1\n","    x = torch.linspace(0, steps, steps)\n","    alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n","    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n","    alphas = alphas_cumprod[1:] / alphas_cumprod[:-1]\n","    return torch.clip(alphas, 0.001, 1)\n","\n","\n","# normalization functions\n","def unnormalize_to_zero_to_one(t):\n","    return (t + 1) * 0.5\n","\n","\n","# DDPM implementation\n","class Diffusion(nn.Module):\n","    def __init__(\n","        self,\n","        model,\n","        *,\n","        image_size,\n","        channels=3,\n","        timesteps=1000,\n","    ):\n","        super().__init__()\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.channels = channels\n","        self.image_size = image_size\n","        self.model = model.to(self.device)\n","        self.model = model\n","        self.num_timesteps = int(timesteps)\n","\n","        \"\"\"\n","        Initializes the diffusion process.\n","            1. Setup the schedule for the diffusion process.\n","            2. Define the coefficients for the diffusion process.\n","        Args:\n","            model: The model to use for the diffusion process.\n","            image_size: The size of the images.\n","            channels: The number of channels in the images.\n","            timesteps: The number of timesteps for the diffusion process.\n","        \"\"\"\n","        ## TODO: Implement the initialization of the diffusion process ##\n","        # 1. define the scheduler here\n","        # 2. pre-compute the coefficients for the diffusion process\n","\n","\n","        self.timesteps = timesteps\n","        self.model = model.to(self.device)\n","\n","        self.alphas = cosine_schedule(self.num_timesteps).to(self.device)\n","        self.one_minus_alphas = (1.0 - self.alphas).to(self.device)\n","        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(self.device)\n","        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1,0), value=1.0).to(self.device)\n","\n","        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n","        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n","\n","        self.posterior_mean_coef_xt = (\n","            torch.sqrt(self.alphas)\n","            * (1.0 - self.alphas_cumprod_prev)\n","            / (1.0 - self.alphas_cumprod)\n","        )\n","        self.posterior_mean_coef_x0 = (\n","            (1.0 - self.alphas)\n","            * torch.sqrt(self.alphas_cumprod_prev)\n","            / (1.0 - self.alphas_cumprod)\n","        )\n","        self.posterior_variance = (\n","            self.one_minus_alphas\n","            * (1.0 - self.alphas_cumprod_prev)\n","            / (1.0 - self.alphas_cumprod)\n","        )\n","        # ###########################################################\n","\n","    def noise_like(self, shape, device):\n","        \"\"\"\n","        Generates noise with the same shape as the input.\n","        Args:\n","            shape: The shape of the noise.\n","            device: The device on which to create the noise.\n","        Returns:\n","            The generated noise.\n","        \"\"\"\n","        noise = lambda: torch.randn(shape, device=device)\n","        return noise()\n","\n","    # backward diffusion\n","    @torch.no_grad()\n","    def p_sample(self, x, t, t_index):\n","        \"\"\"\n","        Computes the (t_index)th sample from the (t_index + 1)th sample using\n","        the reverse diffusion process.\n","        Args:\n","            x: The sampled image at timestep t_index + 1.\n","            t: 1D tensor of the index of the time step.\n","            t_index: Scalar of the index of the time step.\n","        Returns:\n","            The sampled image at timestep t_index.\n","        \"\"\"\n","        ####### TODO: Implement the p_sample function #######\n","        # sample x_{t-1} from the gaussian distribution wrt. posterior mean and posterior variance\n","        # Hint: use extract function to get the coefficients at time t\n","        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n","        # Begin code here\n","        x = x.to(self.device)\n","        t = t.to(self.device)\n","\n","        alpha_t_cumprod = extract(self.alphas_cumprod, t, x.shape)\n","        sqrt_one_minus_alpha_t_cumprod = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n","\n","        epsilon_t = self.model(x, t)\n","        x_0_hat = (x - sqrt_one_minus_alpha_t_cumprod * epsilon_t) / alpha_t_cumprod.sqrt()\n","        x_0_hat = torch.clamp(x_0_hat, -1.0, 1.0)\n","\n","        # posterior mean\n","        posterior_mean_coef_xt_t = extract(self.posterior_mean_coef_xt, t, x.shape)\n","        posterior_mean_coef_x0_t = extract(self.posterior_mean_coef_x0, t, x.shape)\n","        posterior_mean_t = posterior_mean_coef_xt_t * x + posterior_mean_coef_x0_t * x_0_hat\n","\n","        # posterior variance\n","        posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n","\n","        z = self.noise_like(x.shape, device=x.device)\n","\n","        if t_index == 0:\n","            return posterior_mean_t\n","        else:\n","            return posterior_mean_t + torch.sqrt(posterior_variance_t) * z\n","        # ####################################################\n","\n","    @torch.no_grad()\n","    def p_sample_loop(self, img):\n","        \"\"\"\n","        Passes noise through the entire reverse diffusion process to generate\n","        final image samples.\n","        Args:\n","            img: The initial noise that is randomly sampled from the noise distribution.\n","        Returns:\n","            The sampled images.\n","        \"\"\"\n","        b = img.shape[0]\n","        #### TODO: Implement the p_sample_loop function ####\n","        # 1. loop through the time steps from the last to the first\n","        # 2. inside the loop, sample x_{t-1} from the reverse diffusion process\n","        # 3. clamp and unnormalize the generated image to valid pixel range\n","        # Hint: to get time index, you can use torch.full()\n","\n","        img = img.to(self.device)\n","\n","        for t_index in reversed(range(self.timesteps)):\n","            t = torch.full((b,), t_index, device=self.device, dtype=torch.long)\n","            img = self.p_sample(img, t, t_index)\n","\n","        img = unnormalize_to_zero_to_one(img)\n","        img = img.clamp(0, 1)\n","        return img\n","        # ####################################################\n","\n","    @torch.no_grad()\n","    def sample(self, batch_size):\n","        \"\"\"\n","        Wrapper function for p_sample_loop.\n","        Args:\n","            batch_size: The number of images to sample.\n","        Returns:\n","            The sampled images.\n","        \"\"\"\n","        self.model.eval()\n","        #### TODO: Implement the sample function ####\n","        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n","        device = next(self.model.parameters()).device\n","        img_0 = self.noise_like(\n","            (batch_size, self.channels, self.image_size, self.image_size),\n","            device=device\n","        )\n","        img = self.p_sample_loop(img_0)\n","        return img\n","\n","    # forward diffusion\n","    def q_sample(self, x_0, t, noise):\n","        \"\"\"\n","        Applies alpha interpolation between x_0 and noise to simulate sampling\n","        x_t from the noise distribution.\n","        Args:\n","            x_0: The initial images.\n","            t: 1D tensor containing a batch of time indices to sample at.\n","            noise: The noise tensor to sample from.\n","        Returns:\n","            The sampled images.\n","        \"\"\"\n","        ###### TODO: Implement the q_sample function #######\n","        x_0 = x_0.to(self.device)\n","        t = t.to(self.device)\n","        noise = noise.to(self.device)\n","        sqrt_alpha_t_cumprod = extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n","        sqrt_one_minus_alpha_t_cumprod = extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n","        x_t = sqrt_alpha_t_cumprod * x_0 + sqrt_one_minus_alpha_t_cumprod * noise\n","        return x_t\n","\n","    def p_losses(self, x_0, t, noise):\n","        \"\"\"\n","        Computes the loss for the forward diffusion.\n","        Args:\n","            x_0: The initial images.\n","            t: 1D tensor containing a batch of time indices to compute the loss at.\n","            noise: The noise tensor to use.\n","        Returns:\n","            The computed loss.\n","        \"\"\"\n","        ###### TODO: Implement the p_losses function #######\n","        # define loss function wrt. the model output and the target\n","        # Hint: you can use pytorch built-in loss functions: F.l1_loss\n","        x_0 = x_0.to(self.device)\n","        t = t.to(self.device)\n","        noise = noise.to(self.device)\n","        x_t = self.q_sample(x_0, t, noise)\n","        predicted_noise = self.model(x_t, t)\n","        loss = F.l1_loss(predicted_noise, noise)\n","        return loss\n","        # ####################################################\n","\n","    def forward(self, x_0, noise):\n","        \"\"\"\n","        Acts as a wrapper for p_losses.\n","        Args:\n","            x_0: The initial images.\n","            noise: The noise tensor to use.\n","        Returns:\n","            The computed loss.\n","        \"\"\"\n","        x_0 = x_0.to(self.device)\n","        noise = noise.to(self.device)\n","        b, c, h, w, device, img_size, = *x_0.shape, x_0.device, self.image_size\n","        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n","        ###### TODO: Implement the forward function #######\n","        t = torch.randint(0, self.timesteps, (b,), device=device)\n","        return self.p_losses(x_0, t, noise)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1740384070139,"user":{"displayName":"se park","userId":"08959391448453231724"},"user_tz":300},"id":"Y68ZfV_TIK8z"},"outputs":[],"source":["args = {\n","    \"save_folder\": \"./results/\",\n","    \"data_path\": \"./data/train\",\n","    \"train_steps\": 1000,\n","    \"save_and_sample_every\": 100,\n","    \"fid\": False,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"bjr2GtnJIK8z","outputId":"7037bc62-67fa-4902-af5e-e5245a23631e"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">generous-dream-26</strong> at: <a href='https://wandb.ai/spark_park/DDPM_AFHQ/runs/yoy49m5w' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ/runs/yoy49m5w</a><br> View project at: <a href='https://wandb.ai/spark_park/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250224_074030-yoy49m5w/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.6"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Git/class_10623/hw2/handout/wandb/run-20250224_080114-4shwa0r6</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/spark_park/DDPM_AFHQ/runs/4shwa0r6' target=\"_blank\">desert-pond-34</a></strong> to <a href='https://wandb.ai/spark_park/DDPM_AFHQ' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/spark_park/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/spark_park/DDPM_AFHQ/runs/4shwa0r6' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ/runs/4shwa0r6</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["dataset length: 5153, dataset class: cat\n"]},{"output_type":"stream","name":"stderr","text":["steps:   2%|▏         | 16/1000 [00:07<04:45,  3.45it/s]"]}],"source":["# Train the model: If you use colab T4 to train the model, the training process will probably take 2 hours.\n","train_diffusion(**args, Diffusion=Diffusion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ9IsHdPIK8z","executionInfo":{"status":"aborted","timestamp":1740382186128,"user_tz":300,"elapsed":1,"user":{"displayName":"se park","userId":"08959391448453231724"}}},"outputs":[],"source":["# # Visualize the forward and backward process\n","visualize_diffusion(**args, load_path=\"./results/model.pt\", Diffusion=Diffusion)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"diffusion_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}