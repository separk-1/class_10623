{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1740371590219,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "0SQ2MroAIK8x"
   },
   "outputs": [],
   "source": [
    "# On Kaggle\n",
    "# 1. Attach a GPU (see recitation for details on how to do these steps.)\n",
    "# 2. Enable file persistence (see recitation for details on how to do these steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39543,
     "status": "ok",
     "timestamp": 1740371629763,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "a04b7ffSIK8x",
    "outputId": "220edf90-18e2-4053-d615-e11e0ed181f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Git/class_10623/hw2/handout\n",
      "/content/drive/MyDrive/Git/class_10623/hw2/handout\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this on Colab\n",
    "\n",
    "# # Mount your Google Drive to Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd '/content/drive/MyDrive/Git/class_10623/hw2/handout'\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1740371629803,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "MspMKvOSIK8y"
   },
   "outputs": [],
   "source": [
    "# Fetch the homework files.\n",
    "# On Colab, if you save to your drive (by running %cd path_to_your_project), you only need to do this once.\n",
    "#!wget http://www.cs.cmu.edu/~mgormley/courses/10423/homework/hw2.zip\n",
    "#!unzip hw2.zip\n",
    "#!mv handout/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111531,
     "status": "ok",
     "timestamp": 1740371741333,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "1_Yl0V3oIK8y",
    "outputId": "08216492-808b-42ab-d099-f9e05fb75127"
   },
   "outputs": [],
   "source": [
    "# # Set up the environment\n",
    "# !pip install --quiet -r requirements.txt\n",
    "\n",
    "from utils import train_diffusion, visualize_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740371741342,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "TgqMkV9cIK8y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    \"\"\"\n",
    "    This function abstracts away the tedious indexing that would otherwise have\n",
    "    to be done to properly compute the diffusion equations from lecture. This\n",
    "    is necessary because we train data in batches, while the math taught in\n",
    "    lecture only considers a single sample.\n",
    "    \n",
    "    To use this function, consider the example\n",
    "        alpha_t * x\n",
    "    To compute this in code, we would write\n",
    "        extract(alpha, t, x.shape) * x\n",
    "\n",
    "    Args:\n",
    "        a: 1D tensor containing the value at each time step.\n",
    "        t: 1D tensor containing a batch of time indices.\n",
    "        x_shape: The reference shape.\n",
    "    Returns:\n",
    "        The extracted tensor.\n",
    "    \"\"\"\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def cosine_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Passes the input timesteps through the cosine schedule for the diffusion process\n",
    "    Args:\n",
    "        timesteps: 1D tensor containing a batch of time indices.\n",
    "        s: The strength of the schedule.\n",
    "    Returns:\n",
    "        1D tensor of the same shape as timesteps, with the computed alpha.\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    alphas = alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    return torch.clip(alphas, 0.001, 1)\n",
    "\n",
    "\n",
    "# normalization functions\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "\n",
    "# DDPM implementation\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        image_size,\n",
    "        channels=3,\n",
    "        timesteps=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.model = model\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the diffusion process.\n",
    "            1. Setup the schedule for the diffusion process.\n",
    "            2. Define the coefficients for the diffusion process.\n",
    "        Args:\n",
    "            model: The model to use for the diffusion process.\n",
    "            image_size: The size of the images.\n",
    "            channels: The number of channels in the images.\n",
    "            timesteps: The number of timesteps for the diffusion process.\n",
    "        \"\"\"\n",
    "        ## TODO: Implement the initialization of the diffusion process ##\n",
    "        # 1. define the scheduler here\n",
    "        # 2. pre-compute the coefficients for the diffusion process\n",
    "        self.timesteps = timesteps\n",
    "        self.alphas = cosine_schedule(self.num_timesteps)\n",
    "        self.one_minus_alphas=1.0 - self.alphas \n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        self.posterior_mean_coef_xt = (\n",
    "            torch.sqrt(self.alphas) \n",
    "            * (1.0 - self.alphas_cumprod_prev) \n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef_x0 = (\n",
    "            (1.0 - self.alphas) \n",
    "            * torch.sqrt(self.alphas_cumprod_prev) \n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_variance = (\n",
    "            self.one_minus_alphas \n",
    "            * (1.0 - self.alphas_cumprod_prev) \n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # ###########################################################\n",
    "\n",
    "    def noise_like(self, shape, device):\n",
    "        \"\"\"\n",
    "        Generates noise with the same shape as the input.\n",
    "        Args:\n",
    "            shape: The shape of the noise.\n",
    "            device: The device on which to create the noise.\n",
    "        Returns:\n",
    "            The generated noise.\n",
    "        \"\"\"\n",
    "        noise = lambda: torch.randn(shape, device=device)\n",
    "        return noise()\n",
    "\n",
    "    # backward diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index):\n",
    "        \"\"\"\n",
    "        Computes the (t_index)th sample from the (t_index + 1)th sample using\n",
    "        the reverse diffusion process.\n",
    "        Args:\n",
    "            x: The sampled image at timestep t_index + 1.\n",
    "            t: 1D tensor of the index of the time step.\n",
    "            t_index: Scalar of the index of the time step.\n",
    "        Returns:\n",
    "            The sampled image at timestep t_index.\n",
    "        \"\"\"\n",
    "        ####### TODO: Implement the p_sample function #######\n",
    "        # sample x_{t-1} from the gaussian distribution wrt. posterior mean and posterior variance\n",
    "        # Hint: use extract function to get the coefficients at time t\n",
    "        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n",
    "        # Begin code here\n",
    "        alpha_t_cumprod = extract(self.alphas_cumprod, t, x.shape)\n",
    "        sqrt_one_minus_alpha_t_cumprod = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        epsilon_t = self.model(x, t)\n",
    "        x_0_hat = (x - sqrt_one_minus_alpha_t_cumprod * epsilon_t) / alpha_t_cumprod.sqrt()\n",
    "        x_0_hat = torch.clamp(x_0_hat, -1.0, 1.0)\n",
    "        \n",
    "        # posterior mean\n",
    "        posterior_mean_coef_xt_t = extract(self.posterior_mean_coef_xt, t, x.shape)\n",
    "        posterior_mean_coef_x0_t = extract(self.posterior_mean_coef_x0, t, x.shape)\n",
    "        posterior_mean_t = posterior_mean_coef_xt_t * x + posterior_mean_coef_x0_t * x_0_hat\n",
    "        \n",
    "        # posterior variance\n",
    "        posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
    "        \n",
    "        z = self.noise_like(x.shape, device=x.device)\n",
    "\n",
    "        if t_index == 0:\n",
    "            return posterior_mean_t\n",
    "        else:\n",
    "            return posterior_mean_t + torch.sqrt(posterior_variance_t) * z\n",
    "        # ####################################################\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, img):\n",
    "        \"\"\"\n",
    "        Passes noise through the entire reverse diffusion process to generate\n",
    "        final image samples.\n",
    "        Args:\n",
    "            img: The initial noise that is randomly sampled from the noise distribution.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        b = img.shape[0]\n",
    "        #### TODO: Implement the p_sample_loop function ####\n",
    "        # 1. loop through the time steps from the last to the first\n",
    "        # 2. inside the loop, sample x_{t-1} from the reverse diffusion process\n",
    "        # 3. clamp and unnormalize the generated image to valid pixel range\n",
    "        # Hint: to get time index, you can use torch.full()\n",
    "        for t_index in reversed(range(self.timesteps)):\n",
    "            t = torch.full((b,), t_index, device=img.device, dtype=torch.long)\n",
    "            img = self.p_sample(img, t, t_index)\n",
    "\n",
    "        img = unnormalize_to_zero_to_one(img)\n",
    "        img = img.clamp(0, 1)\n",
    "        return img\n",
    "        # ####################################################\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Wrapper function for p_sample_loop.\n",
    "        Args:\n",
    "            batch_size: The number of images to sample.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        #### TODO: Implement the sample function ####\n",
    "        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n",
    "        device = next(self.model.parameters()).device\n",
    "        img_0 = self.noise_like(\n",
    "            (batch_size, self.channels, self.image_size, self.image_size),\n",
    "            device=device\n",
    "        )\n",
    "        img = self.p_sample_loop(img_0)\n",
    "        return img\n",
    "\n",
    "    # forward diffusion\n",
    "    def q_sample(self, x_0, t, noise):\n",
    "        \"\"\"\n",
    "        Applies alpha interpolation between x_0 and noise to simulate sampling\n",
    "        x_t from the noise distribution.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            t: 1D tensor containing a batch of time indices to sample at.\n",
    "            noise: The noise tensor to sample from.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        ###### TODO: Implement the q_sample function #######\n",
    "        sqrt_alpha_t_cumprod = extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n",
    "        sqrt_one_minus_alpha_t_cumprod = extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        x_t = sqrt_alpha_t_cumprod * x_0 + sqrt_one_minus_alpha_t_cumprod * noise\n",
    "        return x_t\n",
    "\n",
    "    def p_losses(self, x_0, t, noise):\n",
    "        \"\"\"\n",
    "        Computes the loss for the forward diffusion.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            t: 1D tensor containing a batch of time indices to compute the loss at.\n",
    "            noise: The noise tensor to use.\n",
    "        Returns:\n",
    "            The computed loss.\n",
    "        \"\"\"\n",
    "        ###### TODO: Implement the p_losses function #######\n",
    "        # define loss function wrt. the model output and the target\n",
    "        # Hint: you can use pytorch built-in loss functions: F.l1_loss\n",
    "        x_t = self.q_sample(x_0, t, noise)\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        loss = F.l1_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "        # ####################################################\n",
    "\n",
    "    def forward(self, x_0, noise):\n",
    "        \"\"\"\n",
    "        Acts as a wrapper for p_losses.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            noise: The noise tensor to use.\n",
    "        Returns:\n",
    "            The computed loss.\n",
    "        \"\"\"\n",
    "        b, c, h, w, device, img_size, = *x_0.shape, x_0.device, self.image_size\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        ###### TODO: Implement the forward function #######\n",
    "        t = torch.randint(0, self.timesteps, (b,), device=device)\n",
    "        return self.p_losses(x_0, t, noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1740371741343,
     "user": {
      "displayName": "se park",
      "userId": "08959391448453231724"
     },
     "user_tz": 300
    },
    "id": "Y68ZfV_TIK8z"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"save_folder\": \"./results/\",\n",
    "    \"data_path\": \"./data/train\",\n",
    "    \"train_steps\": 1000,\n",
    "    \"save_and_sample_every\": 100,\n",
    "    \"fid\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "bjr2GtnJIK8z",
    "outputId": "4d6d5241-1a87-402f-eed6-fe2308af6905"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msparkk\u001b[0m (\u001b[33mspark_park\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\My Drive\\Git\\class_10623\\hw2\\handout\\wandb\\run-20250223_233907-g7lstqus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/spark_park/DDPM_AFHQ/runs/g7lstqus' target=\"_blank\">crimson-dragon-8</a></strong> to <a href='https://wandb.ai/spark_park/DDPM_AFHQ' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/spark_park/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/spark_park/DDPM_AFHQ/runs/g7lstqus' target=\"_blank\">https://wandb.ai/spark_park/DDPM_AFHQ/runs/g7lstqus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 5153, dataset class: cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:   0%|          | 0/1000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'Dataset.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model: If you use colab T4 to train the model, the training process will probably take 2 hours.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_diffusion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, Diffusion\u001b[38;5;241m=\u001b[39mDiffusion)\n",
      "File \u001b[1;32mg:\\My Drive\\Git\\class_10623\\hw2\\handout\\utils.py:118\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[1;34m(train_steps, save_and_sample_every, fid, save_folder, data_path, load_path, unet_dim_mults, image_size, batch_size, data_class, time_steps, unet_dim, learning_rate, dataloader_workers, Diffusion)\u001b[0m\n\u001b[0;32m     83\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     84\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDPM_AFHQ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     85\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     name\u001b[38;5;241m=\u001b[39msave_folder\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    102\u001b[0m _, _, trainer \u001b[38;5;241m=\u001b[39m setup_diffusion(train_steps\u001b[38;5;241m=\u001b[39mtrain_steps, \n\u001b[0;32m    103\u001b[0m                                 save_and_sample_every\u001b[38;5;241m=\u001b[39msave_and_sample_every, \n\u001b[0;32m    104\u001b[0m                                 fid\u001b[38;5;241m=\u001b[39mfid, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m                                 dataloader_workers\u001b[38;5;241m=\u001b[39mdataloader_workers, \n\u001b[0;32m    116\u001b[0m                                 Diffusion\u001b[38;5;241m=\u001b[39mDiffusion)\n\u001b[1;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32mg:\\My Drive\\Git\\class_10623\\hw2\\handout\\trainer.py:170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every):\n\u001b[1;32m--> 170\u001b[0m     data_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     data_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(data_1)\n\u001b[0;32m    173\u001b[0m     data_1, data_2 \u001b[38;5;241m=\u001b[39m data_1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), data_2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mg:\\My Drive\\Git\\class_10623\\hw2\\handout\\trainer.py:17\u001b[0m, in \u001b[0;36mcycle\u001b[1;34m(dl)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcycle\u001b[39m(dl):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[0;32m     18\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\82102\\anaconda3\\envs\\diffusion_env\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'Dataset.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# Train the model: If you use colab T4 to train the model, the training process will probably take 2 hours.\n",
    "train_diffusion(**args, Diffusion=Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ9IsHdPIK8z"
   },
   "outputs": [],
   "source": [
    "# # Visualize the forward and backward process\n",
    "visualize_diffusion(**args, load_path=\"./results/model.pt\", Diffusion=Diffusion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
